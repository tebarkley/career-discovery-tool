{
 "metadata": {
  "name": "",
  "signature": "sha256:784e00734ba1b287d92292326409f4c916a09c407bd46eb7e295c64895c6fab6"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk\n",
      "from nltk.corpus import brown\n",
      "import re\n",
      "from nltk.corpus import stopwords\n",
      "from nltk.corpus import wordnet as wn\n",
      "from nltk.stem import WordNetLemmatizer\n",
      "from string import punctuation\n",
      "from os import path, pardir\n",
      "import sys\n",
      "import pandas as pd\n",
      "from pandas import DataFrame, Series\n",
      "import numpy as np\n",
      "from bs4 import BeautifulSoup\n",
      "import re\n",
      "from collections import defaultdict"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data = pd.io.pickle.read_pickle(path.join(pardir, 'data', 'job_descriptions.pkl'))\n",
      "data.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Create a simplified df with just the DID and the description"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data.columns"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def asciiEncode(row):\n",
      "    return row.encode('ascii')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "post = data[['DID', 'JobDescription']]\n",
      "post.set_index('DID', inplace=True)\n",
      "post.columns = ['raw_text']\n",
      "# post['raw_text'] = post.raw_text.map(lambda x: x.encode('ascii', 'ignore'))\n",
      "# post.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Add a column with better-formed HTML"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "post['html'] = post.raw_text.map(lambda x: BeautifulSoup(x).getText())\n",
      "# post.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Add a column with just the text (adapted from: http://stackoverflow.com/questions/10524387/beautifulsoup-get-text-does-not-strip-all-tags-and-javascript)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def visible_text(element):\n",
      "    if element.parent.name in ['style', 'script', '[document]', 'head', 'title']:\n",
      "        return ''\n",
      "    #get rid of tags\n",
      "    result = re.sub('<br>|<br \\/>|<p\\/?>|<\\/?li>', '\\n', str(element.encode('ascii', 'ignore')))\n",
      "    result = re.sub('<.*?>', ' ', result, flags=re.DOTALL)\n",
      "    #add single quotes\n",
      "    result = re.sub('&rsquo;', \"'\", result)\n",
      "    #get rid of extra spaces, html spaces, and bullets\n",
      "    result = re.sub(' {2,}|&nbsp;|&bull;', ' ', result)\n",
      "    #replace multiple newlines with single newline\n",
      "    result = re.sub('\\s{2,}|\\t', '\\n', result)\n",
      "    #add in double quotes\n",
      "    result = re.sub('&ldquo;', '\"', result)\n",
      "    result = re.sub('&rdquo;', '\"', result)\n",
      "    result = re.sub('&rdquo;', '\"', result)\n",
      "    return result"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def rawToText(description):\n",
      "    soup = BeautifulSoup(description)\n",
      "    texts = soup.findAll(text=True)\n",
      "    visible_elements = [visible_text(elem) for elem in texts]\n",
      "    text = ''.join(visible_elements)\n",
      "    return text"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "post['text'] = post.raw_text.apply(rawToText)\n",
      "# post.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Add a column for a list of sentences"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "post['sents'] = post.text.map(lambda x: nltk.sent_tokenize(x))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Further split sentences on new lines"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def splitNewLine(sents):\n",
      "    final_sents = []\n",
      "    for sent in sents:\n",
      "        sub_sents = sent.split('\\n')\n",
      "        for sub in sub_sents:\n",
      "            if sub != '':\n",
      "                final_sents.append(sub)\n",
      "    return final_sents\n",
      "\n",
      "def splitSemicolon(sents):\n",
      "    final_sents = []\n",
      "    for sent in sents:\n",
      "        sub_sents = sent.split(';')\n",
      "        for sub in sub_sents:\n",
      "            if sub != '':\n",
      "                final_sents.append(sub)\n",
      "    return final_sents"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "post['sents'] = post['sents'].map(splitNewLine)\n",
      "post['sents'] = post['sents'].map(splitSemicolon)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "post.sents[51]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Add a column with list (sentences) of lists (words)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "post['words'] = post.sents.map(lambda x: [nltk.word_tokenize(sent) for sent in x])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Training on all brown sentences, excluding news corpus\n",
      "brown_tagged_sents = brown.tagged_sents(categories=['news', 'adventure', 'belles_lettres', 'editorial', 'fiction', 'government', 'hobbies',\n",
      "'humor', 'learned', 'lore', 'mystery', 'religion', 'reviews', 'romance',\n",
      "'science_fiction'])\n",
      "\n",
      "job_verbs = [[('Maintain', 'VB')], [('Maintains', 'VBZ')], [('Maintaining', 'VBG')],\n",
      "             [('Ensure', 'VB')], [('Ensures', 'VBZ')], [('Ensuring', 'VBG')],\n",
      "             [('Perform', 'VB')], [('Performs', 'VBZ')], [('Performing', 'VBG')],\n",
      "             [('Manage', 'VB')], [('Manages', 'VBZ')], [('Managing', 'VBG')],\n",
      "             [('Provide', 'VB')], [('Provides', 'VBZ')], [('Providing', 'VBG')],\n",
      "             [('Assist', 'VB')], [('Assists', 'VBZ')], [('Assisting', 'VBG')],\n",
      "             [('Prepare', 'VB')], [('Prepares', 'VBZ')], [('Preparing', 'VBG')],\n",
      "             [('Participate', 'VB')], [('Participates', 'VBZ')], [('Participating', 'VBG')],\n",
      "             [('Develop', 'VB')], [('Develops', 'VBZ')], [('Developing', 'VBG')],\n",
      "             [('Understand', 'VB')], [('Understands', 'VBZ')], #Understanding? N\n",
      "             [('Communicate', 'VB')], [('Communicates', 'VBZ')], [('Communicating', 'VBG')],\n",
      "             [('Coordinate', 'VB')], [('Coordinates', 'VBZ')], [('Coordinating', 'VBG')],\n",
      "             [('Complete', 'VB')], [('Completes', 'VBZ')],\n",
      "             [('Create', 'VB')], [('Creates', 'VBZ')], [('Creating', 'VBG')],\n",
      "             [('Review', 'VB')], [('Reviews', 'VBZ')],\n",
      "             [('Receive', 'VB')], [('Receives', 'VBZ')], [('Receiving', 'VBG')],\n",
      "             [('Analyze', 'VB')], [('Analyzes', 'VBZ')], [('Analyzing', 'VBG')],\n",
      "             [('Demonstrate', 'VB')], [('Demonstrates', 'VBZ')], [('Demonstrating', 'VBG')],\n",
      "             [('Interact', 'VB')], [('Interacts', 'VBZ')], [('Interacting', 'VBG')],\n",
      "             [('Adhere', 'VB')], [('Adheres', 'VBZ')], [('Adhering', 'VBG')],\n",
      "             [('Visit', 'VB')], [('Visiting', 'VBG')],\n",
      "             [('Lead', 'VB')], [('Leads', 'VBZ')], # Leading? adj\n",
      "             [('Monitor', 'VB')], [('Monitors', 'VBZ')], [('Monitoring', 'VBG')],\n",
      "             [('Sell', 'VB')], [('Sells', 'VBZ')], [('Selling', 'VBG')]\n",
      "             ]\n",
      "\n",
      "all_tagged_sents = job_verbs + brown_tagged_sents\n",
      "\n",
      "def create_data_sets():\n",
      "    size = int(len(all_tagged_sents) * 0.9)\n",
      "    train_sents = all_tagged_sents[:size]\n",
      "    test_sents = all_tagged_sents[size:]\n",
      "    return train_sents, test_sents\n",
      "train_sents, test_sents = create_data_sets()\n",
      "\n",
      "def build_backoff_tagger (train_sents):\n",
      "    t0 = nltk.DefaultTagger('NN')\n",
      "    t1 = nltk.UnigramTagger(train_sents, backoff=t0)\n",
      "    t2 = nltk.BigramTagger(train_sents, backoff=t1)\n",
      "    return t2\n",
      "ngram_tagger = build_backoff_tagger(train_sents)\n",
      "print \"%0.3f\" % ngram_tagger.evaluate(test_sents)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Do default pos tagging on the words"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "post['ngram_tags'] = post.words.map(lambda x: [ngram_tagger.tag(word) for word in x])\n",
      "\n",
      "# post.words.map(lambda x: [nltk.pos_tag(sent) for sent in x])\n",
      "post.ngram_tags                    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "flat_tagged_sents = [item for sublist in post.ngram_tags.values for item in sublist]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def freq_normed_bigrams(sents):\n",
      "    wnl = WordNetLemmatizer()\n",
      "    \n",
      "    tagged_POS_sents = [item for sublist in sents for item in sublist]\n",
      "    \n",
      "    normed_tagged_words = [wnl.lemmatize(word[0].lower()) for sent in tagged_POS_sents\n",
      "                           for word in sent \n",
      "                           if word[0].lower() not in nltk.corpus.stopwords.words('english')\n",
      "                           and word[0] not in punctuation\n",
      "                           and not re.search(r'''^[\\.,;\"'?!():\\-_`]+$''', word[0])]\n",
      "#                            and word[1].startswith('N')]\n",
      "\n",
      "    top_bigrams = nltk.FreqDist(nltk.bigrams(normed_tagged_words)).items()[:40]\n",
      "    print '%-26s' % 'Word 1', '%-26s' % 'Word 2', '%-26s' % 'Freq'\n",
      "    for ((word1, word2), freq) in top_bigrams:\n",
      "        print '%-26s' % word1, '%-26s' % word2, '%-26s' % freq"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "freq_normed_bigrams(post.ngram_tags.values)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_verbs(tagged_sents):\n",
      "    verbs = []\n",
      "    for sent in tagged_sents:\n",
      "        for word in sent:\n",
      "            if word[1].startswith('VB'):\n",
      "                verbs.append(word[0].lower())\n",
      "    return nltk.FreqDist(verbs).keys()[:20] # use 20 most common verbs\n",
      "\n",
      "def vp_chunker(sentence):\n",
      "    grammar = r'''\n",
      "        VP:\n",
      "            {<V.*><AT|IN|JJ|CD>+<N.*>+} \n",
      "        '''\n",
      "    cp = nltk.RegexpParser(grammar)\n",
      "    return cp.parse(sentence)\n",
      "\n",
      "def test_chunker(sentence, verb_list):\n",
      "    tree = vp_chunker(sentence)\n",
      "    VPs = []\n",
      "    for subtree in tree.subtrees():\n",
      "        if subtree.node == 'VP': \n",
      "            leaf = [leaf[0] for leaf in subtree.leaves()]\n",
      "            for verb in verb_list:\n",
      "                if verb in leaf:\n",
      "                    VPs.append(' '.join(leaf))\n",
      "    return VPs\n",
      "\n",
      "def fd_chunked(sents):    \n",
      "    tagged_POS_sents = [item for sublist in sents for item in sublist]\n",
      "    VPs = []\n",
      "    verb_list = get_verbs(tagged_POS_sents)\n",
      "    for sent in tagged_POS_sents:\n",
      "        for verb in verb_list:\n",
      "            obj = test_chunker(sent, verb_list)\n",
      "            if obj:\n",
      "                for w in obj:\n",
      "                    VPs.append(w)\n",
      "    fd_VPs = nltk.FreqDist(VPs).items()[:60]\n",
      "    \n",
      "    print '%-36s' % 'Chunk','%-26s' % 'Freq'\n",
      "    for (chunk, freq) in fd_VPs:\n",
      "        print '%-36s' % chunk, '%-26s' % freq"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fd_chunked(post.ngram_tags.values)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def freq_normed_unigrams(sents):\n",
      "    wnl = WordNetLemmatizer() # to get word stems\n",
      "    \n",
      "    tagged_POS_sents = [item for sublist in sents for item in sublist]\n",
      "    \n",
      "    normed_tagged_words = [wnl.lemmatize(word[0].lower()) for sent in tagged_POS_sents\n",
      "                           for word in sent \n",
      "                           if word[0].lower() not in nltk.corpus.stopwords.words('english')\n",
      "                           and word[0] not in punctuation # remove punctuation\n",
      "                           and not re.search(r'''^[\\.,;\"'?!():\\-_`]+$''', word[0])\n",
      "                           and word[1].startswith('N')]  # include only nouns\n",
      "\n",
      "    top_normed_unigrams = nltk.FreqDist(normed_tagged_words).keys()[:40]\n",
      "    return top_normed_unigrams\n",
      "\n",
      "def categories_from_hypernyms(sents):\n",
      "    termlist = freq_normed_unigrams(sents) # get top unigrams\n",
      "    hypterms = []\n",
      "    hypterms_dict = defaultdict(list)\n",
      "    for term in termlist:                  # for each term\n",
      "        s = wn.synsets(term.lower(), 'n')  # get its nominal synsets\n",
      "        for syn in s:                      # for each lemma synset\n",
      "            for hyp in syn.hypernyms():    # It has a list of hypernyms\n",
      "                hypterms = hypterms + [hyp.name]      # Extract the hypernym name and add to list\n",
      "                hypterms_dict[hyp.name].append(term)  # Extract examples and add them to dict\n",
      "    hypfd = nltk.FreqDist(hypterms)\n",
      "    for (name, count) in hypfd.items()[:25]:\n",
      "        print name, '({0})'.format(count)\n",
      "        print '\\t', ', '.join(set(hypterms_dict[name]))\n",
      "        print"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "categories_from_hypernyms(post.ngram_tags.values)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "[item for sublist in post.ngram_tags.values for item in sublist]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def freq_dist(sents):\n",
      "    tagged_POS_sents = [item for sublist in sents for item in sublist]\n",
      "    \n",
      "    normed_tagged_words = [sent[0] for sent in tagged_POS_sents]\n",
      "    \n",
      "    # frequency distribution of first words in sentences\n",
      "    return nltk.FreqDist(normed_tagged_words).items()\n",
      "\n",
      "freq_dist(post.ngram_tags.values)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "post.to_pickle(path.join(pardir, 'data','preprocessed_text_nov24.pkl'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}